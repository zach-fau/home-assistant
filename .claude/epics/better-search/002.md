---
name: Test and validate latency improvements
status: open
created: 2025-10-30T20:40:10Z
updated: 2025-10-30T20:40:10Z
github: Will be updated when synced to GitHub
depends_on: [001]
parallel: false
conflicts_with: []
---

# Task: Test and validate latency improvements

## Description
Measure and validate that the immediate acknowledgment feature meets all success criteria from the PRD. This includes latency benchmarks (<500ms TTFS), phrase variety, parallel execution, and overall user experience improvements.

**Goal**: Confirm the implementation works correctly and delivers the promised 4-7s → <1s perceived latency improvement.

## Acceptance Criteria
- [ ] Time to first sound (TTFS) measured at <500ms (10 test runs)
- [ ] All 6 phrases used at least once in 20 search queries
- [ ] Search still completes successfully while acknowledgment plays
- [ ] No errors or warnings in logs during testing
- [ ] Timestamps logged show acknowledgment before search completion
- [ ] Phrase distribution is reasonably random (no obvious bias)
- [ ] User (Gyatso) confirms improved responsiveness vs. baseline
- [ ] Documentation of test results in task notes or commit message

## Technical Details

### Testing Methodology

#### 1. Latency Test (TTFS < 500ms)
**Approach**: Log timestamps at key points
```python
# Add temporary logging in event handler (or check existing logs)
t0 = time.time()
# ... acknowledgment code ...
t1 = time.time()
logger.info(f"TTFS: {(t1-t0)*1000:.2f}ms")
```

**Process**:
1. Trigger 10 web searches via voice
2. Capture log timestamps for each
3. Calculate average TTFS
4. Verify all runs < 500ms

**Expected Result**: Average ~100-300ms (TTS queue + phrase selection)

#### 2. Variety Test (All 6 phrases used)
**Approach**: Count phrase occurrences in logs
```bash
# Extract acknowledgment phrases from logs
grep "Immediate acknowledgment" bot.log | awk '{print $4}' | sort | uniq -c
```

**Process**:
1. Trigger 20 web searches
2. Log phrase used for each
3. Verify all 6 phrases appeared at least once
4. Check distribution is reasonably balanced (no phrase >50% or <5%)

**Expected Result**: Each phrase used 1-5 times in 20 trials

#### 3. Parallel Execution Test
**Approach**: Verify search completes successfully
**Process**:
1. Trigger web search via voice
2. Verify acknowledgment plays immediately
3. Wait for search results
4. Verify results are correct and complete
5. Check logs show no blocking/race conditions

**Expected Result**: Search completes in 3-5s (unchanged from baseline)

#### 4. Error Handling Test
**Approach**: Test edge cases
**Process**:
1. Test with network issues (disconnect WiFi briefly)
2. Test with multiple concurrent searches
3. Test with very long queries
4. Verify no crashes or hangs

**Expected Result**: Graceful degradation (acknowledgment may fail but search continues)

### Metrics to Collect
| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| TTFS (avg) | <500ms | Log timestamps |
| TTFS (max) | <1000ms | Log timestamps |
| Phrase variety | 6/6 used | Log grep + count |
| Search success rate | 100% | Result validation |
| Concurrent handling | No errors | Log review |

### Files to Analyze
- Bot logs (stdout or log file)
- `functions/__init__.py` (review logging)
- Test notes (document findings)

## Dependencies
- [ ] Task 001 complete - ✅ Acknowledgment code implemented
- [ ] Bot running successfully
- [ ] Web interface accessible
- [ ] Tavily API key configured

## Effort Estimate
- Size: S (testing and validation)
- Hours: 0.5 hours (30 minutes)
- Parallel: false (depends on Task 001)

**Breakdown**:
- Setup test environment: 5 min
- Run latency tests (10 trials): 10 min
- Run variety tests (20 trials): 10 min
- Run parallel/error tests: 5 min
- Analyze logs and document: 10 min

## Definition of Done
- [ ] All 4 test categories completed (latency, variety, parallel, errors)
- [ ] All acceptance criteria met and documented
- [ ] Test results logged (either in task notes, commit message, or separate doc)
- [ ] TTFS confirmed <500ms average
- [ ] Phrase variety confirmed (all 6 used)
- [ ] No blocking or errors observed
- [ ] Gyatso confirms improved UX vs. baseline (4-7s silence)
- [ ] Ready to proceed to Task 003 (documentation)

## Testing Script (Optional)
**Manual test sequence**:
```
Test 1: Latency (10 runs)
1. "What's the weather in Seattle?"
2. "What's the capital of France?"
3. "Who won the game last night?"
4. "Best coffee shops in Portland?"
5. "How do I make sourdough bread?"
6. "What's the population of Tokyo?"
7. "Latest news on AI?"
8. "How tall is Mount Everest?"
9. "What time is it in London?"
10. "Best restaurants near me?"

Test 2: Variety (20 runs)
Repeat queries 1-10 twice, log phrases used

Test 3: Parallel
Trigger search, verify results arrive correctly

Test 4: Errors
Test edge cases (network issues, concurrent searches)
```

**Success Criteria**: All tests pass, metrics within targets

## Expected Findings
**Positive**:
- TTFS consistently <300ms (TTS queue is fast)
- Phrases evenly distributed (random.choice works)
- Search unaffected (parallel execution works)
- User reports dramatic improvement

**Potential Issues** (and mitigations):
- TTFS occasionally >500ms → Use shorter phrases ("Searching")
- Phrase bias → Verify random.seed() not called
- Search delays → Check Tavily API status
- TTS failures → Add try/except around queue_frame()
