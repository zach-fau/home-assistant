---
name: Performance Tuning
status: open
created: 2025-10-29T00:19:22Z
updated: 2025-10-29T00:19:22Z
github: null
depends_on: [3]
parallel: true
conflicts_with: [4]
---

# Task: Performance Tuning

## Description

Optimize the voice pipeline to meet latency targets (<800ms p95) by tuning VAD configuration, measuring end-to-end performance, and adjusting service settings. Focus on achieving natural, responsive conversation flow without sacrificing accuracy or quality.

**Target:** 500-800ms end-to-end latency for natural conversation feel.

## Acceptance Criteria

- [ ] VAD stop threshold optimized (balance responsiveness vs false interruptions)
- [ ] End-to-end latency measured systematically
- [ ] p95 latency <800ms achieved
- [ ] Average latency <600ms achieved
- [ ] No noticeable lag in conversation
- [ ] VAD doesn't cut off speech prematurely
- [ ] VAD doesn't wait too long after speech ends
- [ ] Performance benchmarks documented
- [ ] Optimization decisions documented with rationale

## Technical Details

### VAD (Voice Activity Detection) Tuning

**Location:** `bot.py` - SileroVADAnalyzer configuration

**Default Configuration:**
```python
vad = SileroVADAnalyzer(
    stop_secs=0.2  # Wait 200ms after speech stops
)
```

**Tuning Parameters:**

1. **More Responsive (Faster, More Interruptions)**
   ```python
   vad = SileroVADAnalyzer(
       stop_secs=0.1,  # 100ms - very quick
       min_speech_duration=0.1
   )
   ```
   - **Pros:** Feels snappier, less waiting
   - **Cons:** May cut off speech, more false positives

2. **More Patient (Slower, Fewer Interruptions)**
   ```python
   vad = SileroVADAnalyzer(
       stop_secs=0.5,  # 500ms - more patient
       min_speech_duration=0.3
   )
   ```
   - **Pros:** Won't cut off speech, fewer false positives
   - **Cons:** Feels laggy, especially for short responses

3. **Balanced (Recommended Starting Point)**
   ```python
   vad = SileroVADAnalyzer(
       stop_secs=0.2,  # 200ms - middle ground
       min_speech_duration=0.2
   )
   ```

### Latency Measurement

**Components to Measure:**

1. **VAD Detection** (Target: <300ms)
   - Time from speech end to detection

2. **STT Transcription** (Target: <200ms first word)
   - Time to receive first transcript token
   - Deepgram streams in real-time

3. **LLM Processing** (Target: <500ms first token)
   - Time to receive first response token
   - OpenAI GPT-4 Turbo streams

4. **TTS Synthesis** (Target: <200ms first audio)
   - Time to generate first audio chunk
   - Cartesia optimized for low latency

5. **End-to-End** (Target: <800ms p95)
   - Total time from speech end to audio playback start

**Measurement Approach:**

```python
# Add timing instrumentation to bot.py
import time

class LatencyTracker:
    def __init__(self):
        self.timings = {
            "vad_detect": [],
            "stt_first_word": [],
            "llm_first_token": [],
            "tts_first_audio": [],
            "end_to_end": []
        }

    def start(self, event):
        self.start_time = time.time()

    def end(self, event, category):
        elapsed = (time.time() - self.start_time) * 1000  # ms
        self.timings[category].append(elapsed)

    def report(self):
        for category, times in self.timings.items():
            if times:
                avg = sum(times) / len(times)
                p95 = sorted(times)[int(len(times) * 0.95)]
                print(f"{category}: avg={avg:.0f}ms, p95={p95:.0f}ms")
```

### Service-Level Optimizations

**Deepgram (STT):**
```python
stt = DeepgramSTTService(
    api_key=os.getenv("DEEPGRAM_API_KEY"),
    model="nova-2",  # Latest, fastest model
    sample_rate=16000,
    encoding="linear16",
    interim_results=True  # Stream partial results
)
```

**OpenAI (LLM):**
```python
llm = OpenAILLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4-turbo-preview",  # Faster than gpt-4
    temperature=0.7,
    stream=True  # Critical for low latency
)
```

**Cartesia (TTS):**
```python
tts = CartesiaTTSService(
    api_key=os.getenv("CARTESIA_API_KEY"),
    voice_id="a0e99841-438c-4a64-b679-ae501e7d6091",
    model="sonic-english",  # Fastest model
    output_format={
        "container": "raw",
        "encoding": "pcm_s16le",
        "sample_rate": 16000
    }
)
```

### Key Considerations

- **Tradeoffs:** Lower latency may reduce accuracy
- **Network Impact:** Internet speed affects API response times
- **Hardware:** CPU/RAM can bottleneck local processing
- **Service Regions:** Geographic proximity to API servers matters

### Files Affected

- `bot.py` (VAD configuration, optional timing instrumentation)

## Dependencies

### External Dependencies
- [ ] Stable, fast internet connection
- [ ] Access to service documentation for optimization

### Task Dependencies
- [ ] Task 003 (Basic Pipeline Validation) complete

## Effort Estimate

- **Size:** M
- **Hours:** 3-4 hours (iterative testing and measurement)
- **Parallel:** true (can work alongside customization)

## Implementation Notes

### Tuning Process

**Phase 1: Baseline Measurement**
1. Run with default VAD settings (0.2s)
2. Conduct 10 test conversations
3. Measure and record latencies
4. Identify bottlenecks

**Phase 2: VAD Experimentation**
1. Test stop_secs=[0.1, 0.15, 0.2, 0.25, 0.3, 0.5]
2. For each setting, test 5 conversations
3. Note false interruptions and lag perception
4. Select optimal balance

**Phase 3: Service Optimization**
1. Verify streaming enabled for all services
2. Test different Deepgram models if needed
3. Consider GPT-3.5-turbo if GPT-4 too slow
4. Optimize TTS output format

**Phase 4: Validation**
1. Measure final configuration 20+ times
2. Calculate p50, p95, p99 latencies
3. Confirm <800ms p95 target achieved
4. Document final settings

### Testing Script

Run this test for each configuration:

```
Test Conversation:
1. "Hello" [pause] [measure latency]
2. "What's two plus two?" [pause] [measure latency]
3. "Tell me more about that" [pause] [measure latency]
4. "Okay thanks" [pause] [measure latency]

Record:
- Latency for each exchange
- Any false interruptions
- Any noticeable lag
- Subjective responsiveness (1-5 scale)
```

### Troubleshooting Common Issues

**Issue:** High latency (>1 second consistently)
**Solution:** Check internet speed, try different API regions
```bash
# Test internet speed
speedtest-cli

# Check ping to services
ping api.openai.com
ping api.deepgram.com
```

**Issue:** VAD cutting off speech
**Solution:** Increase stop_secs incrementally
```python
vad = SileroVADAnalyzer(stop_secs=0.3)  # More patient
```

**Issue:** VAD waiting too long
**Solution:** Decrease stop_secs
```python
vad = SileroVADAnalyzer(stop_secs=0.15)  # More responsive
```

**Issue:** Inconsistent latency (wide variance)
**Solution:** Check for background network activity, CPU load
```bash
# Monitor network
iftop

# Monitor CPU
htop
```

## Validation Tests

### Test 1: Latency Measurement
```
Run 20 exchanges, measure each:
- p50: _____ ms (target: <600ms)
- p95: _____ ms (target: <800ms)
- p99: _____ ms
Result: PASS/FAIL
```

### Test 2: VAD Responsiveness
```
Test with varying speech patterns:
- Short responses ("Yes")
- Long responses (20+ words)
- Pauses mid-sentence
False interruptions: _____ out of 20
Result: Must be <2 false interruptions
```

### Test 3: Subjective Feel
```
Rate conversation flow:
- Responsiveness: ___/5
- Naturalness: ___/5
- No perceived lag: ___/5
Overall: Must score â‰¥4
Result: PASS/FAIL
```

### Test 4: Stress Test
```
Rapid-fire exchanges (10 quick questions):
Average latency: _____ ms
Consistency: Variance _____ ms
Result: Should maintain performance
```

## Documentation

After optimization, document:

1. **Final Configuration**
   - VAD settings with rationale
   - Service configurations
   - Any model changes

2. **Performance Benchmarks**
   - Latency measurements (p50, p95, p99)
   - Breakdown by component
   - Comparison to baseline

3. **Rejected Approaches**
   - Settings tested but not adopted
   - Reasons for rejection
   - Tradeoffs considered

4. **Known Limitations**
   - Factors affecting performance
   - Conditions where latency degrades
   - Mitigation strategies

## Definition of Done

- [ ] VAD configuration optimized
- [ ] Latency measured systematically (20+ samples)
- [ ] p95 latency <800ms achieved
- [ ] Average latency <600ms achieved
- [ ] Subjective feel is responsive and natural
- [ ] Performance benchmarks documented
- [ ] Optimization decisions documented
- [ ] Configuration checked into version control

## References

- Silero VAD Documentation: https://github.com/snakers4/silero-vad
- Deepgram Performance: https://developers.deepgram.com/docs/performance
- OpenAI Latency Optimization: https://platform.openai.com/docs/guides/latency-optimization
- Cartesia Performance: https://docs.cartesia.ai/performance
- Pipecat Reference: `.claude/docs/pipecat-reference.md`
