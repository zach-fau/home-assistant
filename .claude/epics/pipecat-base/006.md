---
name: Monitoring & Logging
status: open
created: 2025-10-29T00:19:22Z
updated: 2025-10-29T00:19:22Z
github: null
depends_on: [3]
parallel: true
conflicts_with: []
---

# Task: Monitoring & Logging

## Description

Implement comprehensive monitoring and logging to track pipeline performance, debug issues, and monitor API costs. Enable Pipecat's built-in metrics collection and add structured logging for all critical pipeline events.

**Goal:** Gain visibility into system behavior and costs for continuous improvement.

## Acceptance Criteria

- [ ] Structured logging enabled for all pipeline stages
- [ ] Log levels configured appropriately (INFO for production, DEBUG for development)
- [ ] Pipecat metrics collection enabled
- [ ] API usage metrics tracked (tokens, audio minutes, costs)
- [ ] Console output is readable and informative
- [ ] Cost estimation dashboard or script created
- [ ] Performance metrics logged (latency, throughput)
- [ ] Error tracking configured
- [ ] Logs are persistent and searchable

## Technical Details

### Enable Pipeline Metrics

**Location:** `bot.py` - PipelineParams configuration

```python
params = PipelineParams(
    enable_metrics=True,          # Enable pipeline metrics
    enable_usage_metrics=True,    # Track API usage
    log_level="INFO",             # INFO for normal operation
    enable_frame_logging=False,   # DEBUG only - very verbose
)

task = PipelineTask(pipeline, params=params)
```

### Structured Logging Configuration

**Add to bot.py (at top):**

```python
import logging
import sys
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f'logs/pipecat_{datetime.now().strftime("%Y%m%d")}.log')
    ]
)

logger = logging.getLogger(__name__)
```

**Create logs directory:**
```bash
mkdir -p logs
echo "logs/" >> .gitignore  # Don't commit logs
```

### Event Logging

**Key Events to Log:**

```python
@transport.event_handler("on_client_connected")
async def on_client_connected(transport, client):
    logger.info(f"Client connected: {client['id']}")
    logger.info(f"Session started at {datetime.now()}")
    # Track session start time for cost calculation
    session_start = datetime.now()

@transport.event_handler("on_client_disconnected")
async def on_client_disconnected(transport, client):
    logger.info(f"Client disconnected: {client['id']}")
    session_duration = datetime.now() - session_start
    logger.info(f"Session duration: {session_duration}")
    # Log usage metrics
    logger.info(f"Estimated cost: ${estimated_cost:.2f}")

@transport.event_handler("on_error")
async def on_error(transport, error):
    logger.error(f"Pipeline error: {error}", exc_info=True)
```

### Cost Tracking

**Create `scripts/cost_tracker.py`:**

```python
#!/usr/bin/env python3
"""
Track and estimate API costs for Pipecat services.
"""

import json
from datetime import datetime
from typing import Dict

class CostTracker:
    """Track API usage and estimate costs."""

    PRICING = {
        "deepgram": {
            "nova-2": 0.0043,  # per minute of audio
        },
        "openai": {
            "gpt-4-turbo": {
                "input": 0.01 / 1000,   # per token
                "output": 0.03 / 1000,  # per token
            },
            "gpt-3.5-turbo": {
                "input": 0.0005 / 1000,
                "output": 0.0015 / 1000,
            }
        },
        "cartesia": {
            "sonic": 0.10,  # per hour estimate
        }
    }

    def __init__(self):
        self.usage = {
            "deepgram_minutes": 0.0,
            "openai_input_tokens": 0,
            "openai_output_tokens": 0,
            "cartesia_minutes": 0.0,
        }

    def log_deepgram(self, audio_seconds: float):
        """Log Deepgram STT usage."""
        minutes = audio_seconds / 60
        self.usage["deepgram_minutes"] += minutes

    def log_openai(self, input_tokens: int, output_tokens: int):
        """Log OpenAI LLM usage."""
        self.usage["openai_input_tokens"] += input_tokens
        self.usage["openai_output_tokens"] += output_tokens

    def log_cartesia(self, audio_seconds: float):
        """Log Cartesia TTS usage."""
        minutes = audio_seconds / 60
        self.usage["cartesia_minutes"] += minutes

    def estimate_cost(self, model="gpt-4-turbo") -> Dict[str, float]:
        """Calculate estimated costs."""
        costs = {
            "deepgram": self.usage["deepgram_minutes"] * self.PRICING["deepgram"]["nova-2"],
            "openai_input": self.usage["openai_input_tokens"] * self.PRICING["openai"][model]["input"],
            "openai_output": self.usage["openai_output_tokens"] * self.PRICING["openai"][model]["output"],
            "cartesia": (self.usage["cartesia_minutes"] / 60) * self.PRICING["cartesia"]["sonic"],
        }
        costs["total"] = sum(costs.values())
        return costs

    def report(self) -> str:
        """Generate cost report."""
        costs = self.estimate_cost()
        report = f"""
Cost Report ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})
{'='*50}
Deepgram:     ${costs['deepgram']:.4f} ({self.usage['deepgram_minutes']:.1f} min)
OpenAI Input: ${costs['openai_input']:.4f} ({self.usage['openai_input_tokens']:,} tokens)
OpenAI Output: ${costs['openai_output']:.4f} ({self.usage['openai_output_tokens']:,} tokens)
Cartesia:     ${costs['cartesia']:.4f} ({self.usage['cartesia_minutes']:.1f} min)
{'='*50}
TOTAL:        ${costs['total']:.4f}
"""
        return report

    def save(self, filepath: str = "logs/cost_tracking.json"):
        """Save usage data to file."""
        with open(filepath, 'w') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "usage": self.usage,
                "costs": self.estimate_cost()
            }, f, indent=2)

# Global instance
tracker = CostTracker()
```

### Performance Metrics

**Log Key Metrics:**

```python
# In bot.py, add performance tracking
class PerformanceMonitor:
    def __init__(self):
        self.latencies = []
        self.throughput = []

    def log_latency(self, latency_ms: float):
        self.latencies.append(latency_ms)
        if len(self.latencies) % 10 == 0:  # Report every 10 exchanges
            self.report()

    def report(self):
        if self.latencies:
            avg = sum(self.latencies) / len(self.latencies)
            p95 = sorted(self.latencies)[int(len(self.latencies) * 0.95)]
            logger.info(f"Latency - Avg: {avg:.0f}ms, P95: {p95:.0f}ms")

monitor = PerformanceMonitor()
```

### Key Considerations

- **Log Levels:** Use INFO for production, DEBUG only when troubleshooting
- **Log Rotation:** Prevent logs from filling disk
- **Cost Tracking:** Essential for managing API spend
- **Privacy:** Don't log user conversation content in production

### Files Affected

- `bot.py` (logging configuration, metrics enablement)
- `scripts/cost_tracker.py` (new file)
- `logs/` directory (created, gitignored)

## Dependencies

### External Dependencies
- [ ] Disk space for log files
- [ ] Access to API usage dashboards (Deepgram, OpenAI, Cartesia)

### Task Dependencies
- [ ] Task 003 (Basic Pipeline Validation) complete

## Effort Estimate

- **Size:** M
- **Hours:** 2-3 hours
- **Parallel:** true (can work alongside customization/tuning)

## Implementation Notes

### Log Level Guidelines

**INFO:** Normal operation events
- Client connections/disconnections
- Session start/end
- Major pipeline events
- Cost summaries

**DEBUG:** Detailed troubleshooting
- Frame flow details
- Service API calls
- Timing breakdowns

**WARNING:** Recoverable issues
- Retry attempts
- Fallback activations
- Rate limiting

**ERROR:** Critical failures
- Service unavailable
- Pipeline crashes
- Unhandled exceptions

### Cost Monitoring Strategy

**Daily Review:**
```bash
# Check logs for cost estimates
grep "TOTAL:" logs/pipecat_*.log | tail -10

# Review API dashboards
- Deepgram: https://console.deepgram.com/usage
- OpenAI: https://platform.openai.com/usage
- Cartesia: https://cartesia.ai/dashboard/usage
```

**Set Billing Alerts:**
- Deepgram: Alert at $50
- OpenAI: Alert at $50
- Cartesia: Alert at $20

### Troubleshooting Common Issues

**Issue:** Logs too verbose, hard to read
**Solution:** Adjust log level to INFO
```python
logging.basicConfig(level=logging.INFO)
```

**Issue:** Logs filling disk space
**Solution:** Implement log rotation
```python
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    'logs/pipecat.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
```

**Issue:** Can't find specific log entry
**Solution:** Use grep with timestamps
```bash
grep "2025-10-29 14:30" logs/pipecat_*.log
```

**Issue:** Cost tracking inaccurate
**Solution:** Cross-reference with actual API dashboards weekly

## Validation Tests

### Test 1: Logging Enabled
```
Start bot, check for logs:
- Console output: Visible? YES/NO
- Log file created: YES/NO
- Structured format: YES/NO
Result: PASS/FAIL
```

### Test 2: Metrics Collection
```
Run 5-minute session:
- Metrics collected: YES/NO
- Cost estimate generated: YES/NO
- Latency tracked: YES/NO
Result: PASS/FAIL
```

### Test 3: Error Logging
```
Trigger error (disconnect internet mid-session):
- Error logged: YES/NO
- Stack trace captured: YES/NO
- Recoverable: YES/NO
Result: PASS/FAIL
```

### Test 4: Cost Accuracy
```
Compare estimates to actual:
- Deepgram estimate: $_____ actual: $_____
- OpenAI estimate: $_____ actual: $_____
- Within 20% accuracy: YES/NO
Result: PASS/FAIL
```

## Documentation

After implementation, document:

1. **Logging Configuration**
   - Log levels and their purposes
   - Log file locations
   - Rotation policy

2. **Metrics Guide**
   - What metrics are tracked
   - How to interpret them
   - Where to find them

3. **Cost Tracking**
   - How cost estimation works
   - Accuracy expectations
   - How to review actual costs

4. **Troubleshooting**
   - How to search logs
   - Common log patterns
   - Debug mode activation

## Definition of Done

- [ ] Structured logging configured
- [ ] Pipeline metrics enabled
- [ ] Cost tracking implemented
- [ ] Performance monitoring active
- [ ] Logs directory created and gitignored
- [ ] Cost tracker script created
- [ ] Log levels configured appropriately
- [ ] Monitoring documentation written
- [ ] Tested with 30-minute session

## References

- Python Logging: https://docs.python.org/3/library/logging.html
- Pipecat Metrics: https://docs.pipecat.ai/monitoring
- OpenAI Usage Tracking: https://platform.openai.com/usage
- Deepgram Usage: https://console.deepgram.com/usage
